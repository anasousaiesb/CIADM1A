import streamlit as st
import pandas as pd
from datetime import datetime

# URL direta para o arquivo CSV raw no GitHub
# (Verifique se o nome do branch padr√£o 'main' est√° correto para o reposit√≥rio)
# Estrutura: https://raw.githubusercontent.com/<usuario>/<repositorio>/<branch>/<caminho_para_o_arquivo>
ARQUIVO_URL = "https://raw.githubusercontent.com/anasousaiesb/CIADM1A/main/CIADM1A/2020/INMET_CO_DF_A001_BRASILIA_01-01-2020_A_31-12-2020.CSV"

def analisar_frequencia_chuva_fixo_streamlit(url_arquivo, nome_cidade_display, anos_analise_param, limiar_chuva_mm):
    """
    Analisa a frequ√™ncia de eventos extremos de chuva para um arquivo CSV fixo via URL.
    Retorna um dicion√°rio com resultados e mensagens.
    """
    resultados = {
        "mensagens_status": [],
        "contagem_eventos_df": None,
        "mensagem_tendencia": "",
        "periodo_analisado_str": ""
    }
    
    resultados["mensagens_status"].append(f"Tentando carregar dados de: {url_arquivo}")
    coluna_precipitacao_nome_original = 'PRECIPITA√á√ÉO TOTAL, HOR√ÅRIO (mm)'
    coluna_data_nome_original = 'Data' # Assumindo que existe uma coluna 'Data'

    df_completo = None
    try:
        df_bruto = pd.read_csv(
            url_arquivo,
            decimal=',',
            thousands='.',
            encoding='latin1', # Tente 'utf-8' se latin1 falhar ou der problemas com acentua√ß√£o
            na_values=['null', 'NULL', '', 'NA', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', '1.#QNAN', 'N/A', 'NULL', 'NaN', 'nan'],
            skiprows=8 # Pula as primeiras 8 linhas de cabe√ßalho do INMET se existirem
        )
        resultados["mensagens_status"].append(f"Arquivo carregado com sucesso. Colunas encontradas: {df_bruto.columns.tolist()}")

        # Verificar se as colunas esperadas existem
        if coluna_data_nome_original not in df_bruto.columns:
            resultados["mensagens_status"].append(f"ERRO: Coluna '{coluna_data_nome_original}' n√£o encontrada. Verifique o arquivo e os skip_rows.")
            return resultados
        if coluna_precipitacao_nome_original not in df_bruto.columns:
            resultados["mensagens_status"].append(f"ERRO: Coluna '{coluna_precipitacao_nome_original}' n√£o encontrada.")
            return resultados

        # Selecionar e renomear colunas
        df_completo = df_bruto[[coluna_data_nome_original, coluna_precipitacao_nome_original]].copy()
        df_completo.rename(columns={
            coluna_data_nome_original: 'data',
            coluna_precipitacao_nome_original: 'precipitacao_mm'
        }, inplace=True)

        # Converter precipita√ß√£o para num√©rico
        df_completo.loc[:, 'precipitacao_mm'] = pd.to_numeric(df_completo['precipitacao_mm'], errors='coerce')

        # Converter data para datetime
        # Combinar 'data' com 'HORA UTC' se necess√°rio. No arquivo do INMET, 'Data' √© geralmente DD/MM/YYYY.
        # Se 'HORA UTC' for uma coluna e relevante, voc√™ precisaria combin√°-las:
        # Ex: df_completo['timestamp_completo'] = pd.to_datetime(df_completo['data'] + ' ' + df_bruto['HORA UTC'].str.slice(0,2) + ':00:00', format='%d/%m/%Y %H:%M:%S', errors='coerce')
        # Por agora, vamos assumir que a coluna 'Data' √© suficiente ou j√° est√° como datetime.
        try:
            df_completo.loc[:, 'data'] = pd.to_datetime(df_completo['data'], format='%Y/%m/%d', errors='coerce') # Formato comum do INMET AAAA/MM/DD
            if df_completo['data'].isnull().all(): # Se todas as datas forem NaT, tentar outro formato
                df_completo.loc[:, 'data'] = pd.to_datetime(df_completo['data'], format='%d/%m/%Y', errors='coerce') # Formato DD/MM/AAAA
        except Exception as e_dt:
             resultados["mensagens_status"].append(f"AVISO: N√£o foi poss√≠vel converter a coluna 'data' diretamente, tentando inferir. Erro: {e_dt}")
             df_completo.loc[:, 'data'] = pd.to_datetime(df_completo['data'], errors='coerce')


        # Remover linhas onde data ou precipita√ß√£o n√£o puderam ser convertidas
        df_completo.dropna(subset=['data', 'precipitacao_mm'], inplace=True)
        
        if df_completo.empty:
            resultados["mensagens_status"].append("Nenhum dado v√°lido encontrado ap√≥s limpeza inicial.")
            return resultados
            
    except pd.errors.EmptyDataError:
        resultados["mensagens_status"].append(f"Erro: O arquivo em {url_arquivo} parece estar vazio ou n√£o p√¥de ser lido.")
        return resultados
    except Exception as e:
        resultados["mensagens_status"].append(f"Ocorreu um erro ao carregar ou processar o arquivo da URL: {e}")
        return resultados

    resultados["mensagens_status"].append(f"Total de registros v√°lidos carregados para {nome_cidade_display}: {len(df_completo)}")

    # Filtragem por 'anos_analise_param' - para um arquivo de um ano, isso geralmente significa analisar esse ano.
    data_mais_recente_nos_dados = df_completo['data'].max()
    # Se o arquivo √© de um ano espec√≠fico, anos_analise_param > 1 n√£o far√° sentido para *tend√™ncia*,
    # mas o filtro de data ainda pode ser aplicado se o arquivo contiver mais do que o per√≠odo esperado.
    data_inicio_analise = data_mais_recente_nos_dados - pd.DateOffset(years=anos_analise_param) 
    
    df_recente = df_completo[df_completo['data'] >= data_inicio_analise].copy()

    if df_recente.empty:
        resultados["mensagens_status"].append(f"N√£o h√° dados dispon√≠veis para o per√≠odo de an√°lise especificado (√∫ltimos {anos_analise_param} anos dentro do arquivo).")
        return resultados

    min_date_str = df_recente['data'].min().strftime('%Y-%m-%d')
    max_date_str = df_recente['data'].max().strftime('%Y-%m-%d')
    resultados["periodo_analisado_str"] = f"Analisando dados de {min_date_str} at√© {max_date_str}."
    resultados["mensagens_status"].append(resultados["periodo_analisado_str"])
    
    df_recente.loc[:, 'evento_extremo'] = df_recente['precipitacao_mm'] >= limiar_chuva_mm
    df_recente.loc[:, 'ano'] = df_recente['data'].dt.year
    
    # Garante que todos os anos no per√≠odo de an√°lise estejam presentes, mesmo que com 0 eventos.
    ano_min_analise = df_recente['ano'].min()
    ano_max_analise = df_recente['ano'].max()
    
    contagem_eventos_por_ano = df_recente[df_recente['evento_extremo']].groupby('ano').size().reindex(
        range(ano_min_analise, ano_max_analise + 1), fill_value=0
    )
    resultados["contagem_eventos_df"] = contagem_eventos_por_ano.reset_index(name='Numero de Eventos Extremos')

    if contagem_eventos_por_ano.sum() == 0:
        resultados["mensagem_tendencia"] = f"Nenhum evento de chuva extrema (‚â• {limiar_chuva_mm} mm) encontrado para {nome_cidade_display} no per√≠odo de {min_date_str} a {max_date_str}."
        return resultados
    
    # An√°lise de tend√™ncia - limitada para um √∫nico ano de dados
    if len(contagem_eventos_por_ano) == 1:
        num_eventos_ano_unico = contagem_eventos_por_ano.iloc[0]
        ano_unico = contagem_eventos_por_ano.index[0]
        resultados["mensagem_tendencia"] = f"An√°lise para o ano de {ano_unico}: {num_eventos_ano_unico} evento(s) extremo(s) registrado(s). N√£o √© poss√≠vel analisar tend√™ncia de aumento ou diminui√ß√£o com dados de um √∫nico ano."
    elif len(contagem_eventos_por_ano) > 1: # Caso o arquivo contenha sub-per√≠odos que resultem em m√∫ltiplos "anos" na contagem.
        resultados["mensagem_tendencia"] = "M√∫ltiplos per√≠odos anuais encontrados nos dados. A an√°lise de tend√™ncia abaixo considera estes per√≠odos."
        # (A l√≥gica de tend√™ncia anterior pode ser aplicada aqui se fizer sentido para os dados resultantes)
        anos_com_dados = contagem_eventos_por_ano
        diff_eventos = anos_com_dados.diff().dropna()
        if not diff_eventos.empty:
            if (diff_eventos > 0).all():
                resultados["mensagem_tendencia"] += " A frequ√™ncia de eventos extremos aumentou consistentemente."
            # ... (l√≥gica de tend√™ncia anterior) ...
            else:
                 resultados["mensagem_tendencia"] += " A tend√™ncia da frequ√™ncia de eventos extremos √© vari√°vel."
        else:
             resultados["mensagem_tendencia"] += " N√£o h√° varia√ß√£o suficiente para uma an√°lise de tend√™ncia simples."
    else: # Nenhum evento
        resultados["mensagem_tendencia"] = "N√£o foram encontrados eventos extremos para an√°lise de tend√™ncia."
        
    return resultados

# --- Interface Streamlit ---
st.set_page_config(layout="wide", page_title="An√°lise de Chuvas Extremas (Bras√≠lia 2020)")
st.title("üåßÔ∏è An√°lise de Frequ√™ncia de Chuvas Extremas")
st.markdown(f"Esta aplica√ß√£o analisa um arquivo de dados espec√≠fico para **Bras√≠lia (2020)**.")
st.markdown(f"Fonte dos dados: `{ARQUIVO_URL}`")


# Inputs na barra lateral
st.sidebar.header("Par√¢metros de An√°lise")
# O nome da cidade √© fixo pela fonte de dados, mas pode ser usado para display
nome_cidade_fixo = "Bras√≠lia (DF) - Dados de 2020" 
st.sidebar.info(f"Cidade em an√°lise: {nome_cidade_fixo}")


# O par√¢metro 'anos_analise' √© menos relevante para um arquivo de um √∫nico ano,
# mas pode ser mantido se houver inten√ß√£o de filtrar dentro do per√≠odo do arquivo.
# Para este arquivo de 2020, qualquer valor >= 1 resultar√° na an√°lise de 2020.
# Vamos fixar ou tornar menos proeminente.
# anos_analise_input = st.sidebar.number_input("N√∫mero de anos recentes para analisar (dentro do arquivo):", min_value=1, max_value=5, value=1)
st.sidebar.caption("A an√°lise cobrir√° os dados do arquivo carregado (2020).")
anos_analise_input = 1 # Fixo para este caso, j√° que √© um arquivo de 1 ano.

limiar_chuva_mm_input = st.sidebar.number_input("Limiar de chuva extrema (mm):", min_value=0.1, value=50.0, step=0.1, format="%.1f")

col1, col2 = st.columns([1,3]) # Definindo colunas para layout

with col1: # Bot√£o na primeira coluna
    if st.sidebar.button("üîç Analisar Dados Carregados", use_container_width=True, type="primary"):
        with st.spinner(f"Analisando dados para {nome_cidade_fixo}... Por favor, aguarde."):
            st.session_state.resultados_analise_fixo = analisar_frequencia_chuva_fixo_streamlit(
                ARQUIVO_URL, 
                nome_cidade_fixo, 
                anos_analise_input, 
                limiar_chuva_mm_input
            )
    elif 'resultados_analise_fixo' not in st.session_state :
         st.info("‚¨ÖÔ∏è Ajuste o limiar de chuva (se desejar) e clique em 'Analisar Dados Carregados' para come√ßar.")


with col2: # Resultados na segunda coluna
    if 'resultados_analise_fixo' in st.session_state:
        resultados = st.session_state.resultados_analise_fixo
        
        st.subheader(f"Resultados para: {nome_cidade_fixo}")
        if resultados["periodo_analisado_str"]:
             st.caption(resultados["periodo_analisado_str"])

        if resultados["contagem_eventos_df"] is not None and not resultados["contagem_eventos_df"].empty:
            st.markdown("#### Frequ√™ncia Anual de Eventos Extremos")
            
            df_tabela = resultados["contagem_eventos_df"]
            st.dataframe(df_tabela.style.format({"Numero de Eventos Extremos": "{:.0f}"}), use_container_width=True)
            
            if not df_tabela.empty and 'ano' in df_tabela.columns:
                df_grafico = df_tabela.set_index('ano')
                st.bar_chart(df_grafico["Numero de Eventos Extremos"])
            
            if resultados["mensagem_tendencia"]:
                st.markdown("#### An√°lise de Tend√™ncia")
                st.info(resultados["mensagem_tendencia"])
        elif resultados["mensagem_tendencia"]: # Caso n√£o haja eventos, mas haja uma mensagem
             st.info(resultados["mensagem_tendencia"])
        else:
            st.warning("N√£o foram encontrados dados de eventos extremos para exibir ou houve um erro no processamento.")

        with st.expander("Ver Logs de Processamento"):
            for msg in resultados["mensagens_status"]:
                if "ERRO:" in msg:
                    st.error(msg)
                elif "AVISO:" in msg:
                    st.warning(msg)
                else:
                    st.write(msg)
    
st.sidebar.markdown("---")
st.sidebar.markdown("Desenvolvido como assistente virtual.")
